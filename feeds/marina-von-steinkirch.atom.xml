<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>chmod +x singularity.sh</title><link href="http://bt3gl.github.io/" rel="alternate"></link><link href="http://bt3gl.github.io/feeds/marina-von-steinkirch.atom.xml" rel="self"></link><id>http://bt3gl.github.io/</id><updated>2016-08-06T00:00:00-04:00</updated><entry><title>ICYM AI &amp; ML - Week #31 of 2016</title><link href="http://bt3gl.github.io/icym-ai-ml-week-31-of-2016.html" rel="alternate"></link><updated>2016-08-06T00:00:00-04:00</updated><author><name>Marina von Steinkirch</name></author><id>tag:bt3gl.github.io,2016-08-06:icym-ai-ml-week-31-of-2016.html</id><summary type="html">&lt;h2&gt;Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.blackhat.com/docs/us-16/materials/us-16-Wolff-Applied-Machine-Learning-For-Data-Exfil-And-Other-Fun-Topics.pdf"&gt;Black Hat 2016: Applied Machine Learning for Data Exfil and Other Fun Topics&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.blackhat.com/docs/us-16/materials/us-16-Berlin-An-AI-Approach-To-Malware-Similarity-Analysis-Mapping-The-Malware-Genome-With-A-Deep-Neural-Network.pdf"&gt;Black Hat 2016: An AI approach to Malware Similarity Analysis&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=T1O3ikmTEdA&amp;amp;feature=youtu.be&amp;amp;t=10m56s"&gt;Peter Norvig: How Computers Learn&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aiimpacts.org/costs-of-extinction-risk-mitigation/"&gt;Costs of extinction risk mitigation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html"&gt;Written Memories: Understanding, Deriving and Extending the LSTM&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8#.4on1g3268"&gt;(Self-titled) The best explanation of Convolutional Neural Networks on the Internet&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nuit-blanche.blogspot.com/2016/08/secure-group-testing.html"&gt;Secure Group Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.theverge.com/2016/7/13/12172904/facebook-ai-big-sur-machine-learning-prineville-data-center"&gt;Exploring Facebook’s massive, picture-painting AI brain&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/?_fb_noscript=1"&gt;Facebook to open-source AI hardware design&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arstechnica.com/science/2016/07/algorithms-used-to-study-brain-activity-may-be-exaggerating-results/"&gt;Software faults raise questions about the validity of brain studies&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://prefrontal.org/files/posters/Bennett-Salmon-2009.pdf"&gt;Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: An argument for multiple comparisons correction&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sarahjamielewis.com/posts/adversarial-machine-learning.html"&gt;Adversarial Machine Learning for Security&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deepdrive.io/"&gt;DeepDriveself-driving car AI&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://intelligence.org/2016/08/02/2016-summer-program-recap/"&gt;2016 summer program recap&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Events&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/4v58b2/google_brain_will_be_doing_an_ama_in/"&gt;Google Brain will be doing an AMA in August 11st&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://norvig.com/ipython/README.html"&gt;List of IPython (Jupyter) Notebooks by Peter Norvig&lt;/a&gt;.  &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cchio/deep-pwning"&gt;Deep Pwning: Metasploit for machine learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jcjohnson/cnn-benchmarks"&gt;Benchmarks for popular CNN models&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Videos&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=GWn7vD2Ud3M"&gt;Build an Autoencoder in 5 Min&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>ICYM AI &amp; ML - Week #30 of 2016</title><link href="http://bt3gl.github.io/icym-ai-ml-week-30-of-2016.html" rel="alternate"></link><updated>2016-07-30T00:00:00-04:00</updated><author><name>Marina von Steinkirch</name></author><id>tag:bt3gl.github.io,2016-07-30:icym-ai-ml-week-30-of-2016.html</id><summary type="html">&lt;h2&gt;Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky, &lt;em&gt;et al.&lt;/em&gt;, 2014)&lt;/a&gt;. AlexNet.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1409.1556.pdf"&gt;Very Deep Convolutional Networks for large-scale image recognition (Simonyan, &lt;em&gt;et al.&lt;/em&gt;, 2014)&lt;/a&gt;. Image classification.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1207.0580.pdf"&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;. Dropout and regularization.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1503.03832v3.pdf"&gt;FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;.  Metric learning (FaceNet).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1512.03385v1.pdf"&gt;Deep Residual Learning for Image Recognition (He, &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;. Very deep networks (ResNet).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1409.0473v7.pdf"&gt;Neural Machine Translation by jointly learning to align and translate (Bahdanau, &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;. RNNs, LSTMs, GRUs - machine translation with alignment.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1412.5903v5.pdf"&gt;Deep structured output learning for unconstrained text recognition (Jaderberg,  &lt;em&gt;et al.&lt;/em&gt;, 2014)&lt;/a&gt;.  Text recognition.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1512.02595v1.pdf"&gt;Deep Speech 2: End-to-End Speech Recognition in English and Mandarin (Amodei,  &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;. Speech recognition (DeepSpeech 2).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1508.06576v2.pdf"&gt;A Neural Algorithm of Artistic Style (Gatys,  &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;. Artistic style transfer.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1511.08228v3.pdf"&gt;Neural GPUs learn algorithms (Kaiser, &lt;em&gt;et al.&lt;/em&gt;, 2015)&lt;/a&gt;. A Neural GPUs.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.csail.mit.edu/kalyan/AI2_Paper.pdf"&gt;AI2: Training a big data machine to defend (Kalyan, &lt;em&gt;et al.&lt;/em&gt;, 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"&gt;Tensor Flow Whitepaper, (Abadi, &lt;em&gt;et al.&lt;/em&gt;, 2014)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://lvdmaaten.github.io/publications/papers/Torchnet_2016.pdf"&gt;Torchnet: An Open-Source Platform for (Deep) Learning Research, (Collobert, &lt;em&gt;et al.&lt;/em&gt;, 2016)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interesting to see the relation between 1998's LeCun 10^6 transistors and 10^7 pixels in training and then 2012's Krizhevsky 10^9 transistors (and GPU) and 10^14 pixels in training.&lt;/p&gt;
&lt;h2&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html"&gt;Using Keras and Deep Q-Network to Play FlappyBird&lt;/a&gt;. Hands-on on Google DeepMind's Deep Q-Network.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"&gt;Neural Networks, Manifolds, and Topology&lt;/a&gt;. This is a 2-years-old article, but a very well-written high-level explanation of the topology of low-dimensional NNs. "&lt;em&gt;The task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds.&lt;/em&gt;"&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-08-Backprop/"&gt;Calculus on Computational Graphs: Backpropagation&lt;/a&gt;. Backpropagation explaned in a very well-written text.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;. Another hit :).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-01-Visualizing-Representations/"&gt;Visualizing Representations: Deep Learning and Human Beings.&lt;/a&gt; Another Christopher Olah's great post, now on NN's different layers representations, tanging some philosophic aspects of it.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/cnnembed/"&gt;Karpathy's t-SNE visualization of CNN codes.&lt;/a&gt; He takes the 50k ILSVRC 2012 validation images, extracts the 4096-dimensional fc7 CNN features using Caffe and then uses Barnes-Hut t-SNE to compute a 2-dimensional embedding that respects the high-dimensional (L2) distances. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://blogs.nvidia.com/blog/2016/01/12/accelerating-ai-artificial-intelligence-gpus/"&gt;NVIDIA's Accelerating AI with GPUs: A New Computing Model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.facebook.com/posts/580706092103929/lighting-the-way-to-deep-machine-learning/"&gt;Torchnet: Lighting the way to deep machine learning&lt;/a&gt;. "&lt;em&gt;&lt;a href="https://github.com/torchnet/torchnet"&gt;Torchnet&lt;/a&gt; is different from frameworks such as Caffe, Chainer, TensorFlow, and Theano, in that it does not focus on performing efficient inference and gradient computations in deep networks. Instead, Torchnet provides a framework on top of a deep learning framework that makes rapid experimentation easier.&lt;/em&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44921.pdf"&gt;Large-Scale Deep Learning for Intelligent Computer Systems by Jeff Dean&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/blog/big-data/2016/07/understanding-neural-networks-with-tensorflow-playground"&gt;Understanding neural networks with TensorFlow Playground&lt;/a&gt;.  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html"&gt;Karpathy's ConvNetJS viz tool.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Videos&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=JeBkUtYvBBM"&gt;Prof. Adrian Owen on The Search for Consciousness: detecting awareness in the vegetative state (2015)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&amp;lt;3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Ics9CjRSMfc"&gt;Baidu AI Composer&lt;/a&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remember that the hidden layer learns a representation so that the data is linearly separable, so that's is how you do separate a spiral two-dimensional dataset using Tensorflow playground and Convnetjs:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With tanh:&lt;/p&gt;
&lt;p&gt;&lt;img alt="tahn2" height="300px" src="./tensor_flow_playground/tanh2.png" width="400px" /&gt;  &lt;img alt="tahn11" height="300px" src="./tensor_flow_playground/tan1.png" width="400px" /&gt;   &lt;img alt="tan2" height="300px" src="./tensor_flow_playground/tan2.png" width="400px" /&gt;   &lt;img alt="tan3" height="300px" src="./tensor_flow_playground/tan3.png" width="400px" /&gt; &lt;/p&gt;
&lt;p&gt;With ReLU:&lt;/p&gt;
&lt;p&gt;&lt;img alt="relu2" height="300px" src="./tensor_flow_playground/relu2.png" width="400px" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;That's is how you do not separate a spiral two-dimensional dataset:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="linear" height="300px" src="./tensor_flow_playground/linear.png" width="400px" /&gt; &lt;img alt="relu_no" height="300px" src="./tensor_flow_playground/relu_no.png" width="400px" /&gt; &lt;img alt="sigmoid" height="300px" src="./tensor_flow_playground/sigmoid.png" width="400px" /&gt;  &lt;img alt="relu_no2" height="300px" src="./tensor_flow_playground/relu_no2.png" width="400px" /&gt;  &lt;img alt="relu_no3" height="300px" src="./tensor_flow_playground/relu_no3.png" width="400px" /&gt;  &lt;img alt="relu_no4" height="300px" src="./tensor_flow_playground/relu_no4.png" width="400px" /&gt; &lt;/p&gt;</summary></entry><entry><title>ICYM AI &amp; ML - Week #29 of 2016</title><link href="http://bt3gl.github.io/icym-ai-ml-week-29-of-2016.html" rel="alternate"></link><updated>2016-07-23T00:00:00-04:00</updated><author><name>Marina von Steinkirch</name></author><id>tag:bt3gl.github.io,2016-07-23:icym-ai-ml-week-29-of-2016.html</id><summary type="html">&lt;h2&gt;Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1409.4842.pdf"&gt;Going deeper with Convolutions (Szegedy, &lt;em&gt;et al.&lt;/em&gt;, 2014)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=yxxRAHVtafI"&gt;The Science of Talking with Computers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NK6O8CtI2D4"&gt;Megan Smith: Perspectives on artificial intelligence from the White House&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=6eBpjEdgSm0"&gt;NVIDIA Deep Learning Course: Class #1 – Introduction to Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&amp;lt;3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=u2t77mQmJiY"&gt;A genetic algorithm learns how to fight!&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>ICYM AI &amp; ML - Week #28 of 2016</title><link href="http://bt3gl.github.io/icym-ai-ml-week-28-of-2016.html" rel="alternate"></link><updated>2016-07-16T00:00:00-04:00</updated><author><name>Marina von Steinkirch</name></author><id>tag:bt3gl.github.io,2016-07-16:icym-ai-ml-week-28-of-2016.html</id><summary type="html">&lt;h2&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.benjamintd.com/blog/spynet/?utm_campaign=Artificial%2BIntelligence%2BWeekly&amp;amp;utm_medium=web&amp;amp;utm_source=Artificial_Intelligence_Weekly_42"&gt;Teaching an AI to write Python code with Python code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://efavdb.com/deep-learning-with-jupyter-on-aws/"&gt;Starting DL with Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.gc2fcdcce7_216_515"&gt;DIY Deep Learning for Vision:  a Hands-On Tutorial with Caffe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html"&gt;Wide &amp;amp; Deep Learning: Better Together with TensorFlow&lt;/a&gt;. &lt;em&gt;Can we teach computers to learn like humans do, by combining the power of memorization and generalization?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf"&gt;Mastering the Game of Go with Deep Neural Networks and Tree Search&lt;/a&gt;. Basically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Values networks to evaluate board positions and policy networks to select moves.&lt;/li&gt;
&lt;li&gt;Trained with supervised learning from human expert games and reinforcement learning from games of self-play.&lt;/li&gt;
&lt;li&gt;NN playS Go at the level of state-of-art Monte-Carlo tree search that simulate thousands of random games of self-play.&lt;/li&gt;
&lt;li&gt;New search algorithm that combines Monte-Carlo simulation with value and policy network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/pdf/1607.02533v1.pdf"&gt;Adversarial Examples in the Physical World (Kurakin, &lt;em&gt;et al.&lt;/em&gt;, 2016)&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf"&gt;Deep Residual Networks, Kaiming He, Facebook AI Research&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/AlphaGo_IJCAI.pdf"&gt;AlphaGo Presentation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=XkltShNd6XE"&gt;Prof. Jürgen Schmidhuber - True Artificial Intelligence Will Change Everything&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&amp;lt;3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;"All games of perfect information have an optimal value function which determines the outcome of the game".&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://diogenes.greedbag.com/buy/ghost-lanes-0/"&gt;Post-Rock as it best: Ghost Lanes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Ouroboros Statement</title><link href="http://bt3gl.github.io/ouroboros-statement.html" rel="alternate"></link><updated>2016-07-15T00:00:00-04:00</updated><author><name>Marina von Steinkirch</name></author><id>tag:bt3gl.github.io,2016-07-15:ouroboros-statement.html</id><summary type="html">&lt;p&gt;I was seven when I first saw the &lt;em&gt;shiny beige box&lt;/em&gt;. When my uncle finally let me touch the black screen, I was astounded with the list of unfathomable keys that “help” would print. My parents were getting divorced and my reality had just became &lt;strong&gt;40 megabytes of the most ludic journey&lt;/strong&gt;. In the following years, while growing up under a financial struggle in Brazil, the public schools I attended did not have many resources or computers available. Fortuitously, I encountered my town’s marvelous public library and my reality had just become &lt;strong&gt;five floors of endless lustrous pages&lt;/strong&gt;. At that point it was clear that science and the search for knowledge would be the primary fuel of my existence. &lt;/p&gt;
&lt;p&gt;My first project in &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; was in my freshman year in High School; I was finally in a school that had a computer laboratory. I came across Linux and programming languages. The freedom and possibilities of &lt;strong&gt;open-source software&lt;/strong&gt; culminated on me not only fulfilling my dormant technical dreams, but in a philosophical and political way. As a teenager, that was the first time that I found an identity. By the end of my sophomore year I had my own Flash website with sections for science, fiction, and code snippets.  At that time, with a couple of friends that were earnest with the same ideas, I started learning about how a computer could emulates human’s decision-making abilities with &lt;strong&gt;Expert Systems&lt;/strong&gt;. We created &lt;em&gt;“Medico do Lar”&lt;/em&gt; (“Home’s Doctor”), A GUI-based software in &lt;em&gt;Delphi&lt;/em&gt; that would speed up assistance to patients in a precarious health system like the one in our country. Symptoms could be used as inputs and, in a series of tree decisions, the program would print out possible conditions and any recommendation for immediate actions to be taken. We won first place in the &lt;em&gt;Brazilian Science and Technological Science Fair&lt;/em&gt; (MOSTRATEC). At that time already, computers turned to be my major passion; I would have floppy disk decorating my bedroom and &lt;em&gt;RJ45&lt;/em&gt; ethernet connectors as necklaces. I could not be more grateful for how computers opened an entire new world for me on those early years. &lt;/p&gt;
&lt;p&gt;My universe shifted to another direction when Physics fell into my life. Auspiciously, I had just been accepted into the Engineering Department of the best university in Latin America, &lt;strong&gt;University of Sao Paulo&lt;/strong&gt;. It was my freshman year when I heard about scientific research and scholarship opportunities in the Physics department. The possibility of discovering new laws of the Universe was enough to convince me to transfer my major to Physics. I graduated college simulating &lt;strong&gt;Dark matter and Dark energy&lt;/strong&gt; as decaying interacting fluids, with a summer internship in the &lt;strong&gt;NASA's Goddard Space Flight Center&lt;/strong&gt;, and invited to attend the &lt;strong&gt;CERN Winter School on Supergravity, Strings, and Gauge Theory&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;I remember being a kid and asking adults what should I study to become a &lt;em&gt;“Cientista Maluca”&lt;/em&gt; (something like a mad scientist) when I grow up. I got close to an answer when I started my &lt;strong&gt;Ph.D. in Physics at Stony Brook University in New York&lt;/strong&gt;. In the years following, just like the mathematical cadence that governs our bearing, each course and each project that I chose to work on came to reveal a deeper meaning later. When I concluded the &lt;strong&gt;String Theory&lt;/strong&gt; and &lt;strong&gt;Quantum Field Theory&lt;/strong&gt; curriculum, I came into contact with &lt;strong&gt;Group Theory for the Standard Model in Physics&lt;/strong&gt;. The symmetry was so breathtaking that I spent my entire summer on it. In the end I had written a book in the subject, which turned later to be reference for other students around the world. My next step was to understand the experimental aspect of the theory. I started researching &lt;strong&gt;Quark-Gluon Plasma&lt;/strong&gt;, with Dr. Barbara Jacak in the &lt;strong&gt;Relativistic Heavy Ion Collider at Brookhaven National Laboratory&lt;/strong&gt;. Most of my research was based on writing software to simulate the process, and then analysing the results. As a consequence, in my third year I joined the Computational Astrophysics department, studying &lt;strong&gt;Monte Carlo simulations for Neutron Star atmospheres&lt;/strong&gt;. I started working close with the &lt;strong&gt;Los Alamos National Laboratory&lt;/strong&gt; group, and I was granted a postdoctoral scholarship to work there following my graduation.&lt;/p&gt;
&lt;p&gt;At that time I was taking some graduate courses in the computer science department. With Dr. Leman Akoglu’s Machine Learning and Dr. Tamara Berg’s Computational Photography, I was conveyed to the &lt;strong&gt;exquisiteness of Artificial Intelligence&lt;/strong&gt; again. Alan Turing wrote that, &lt;em&gt;“Those who can imagine anything, can create the impossible”&lt;/em&gt;. The history of AI through the last 60 years are a practical example of this wisdom. Particularly in the recent years, with the advances within cognitive science, and in subfields such as deep learning and deep reinforcement learning. The impressive results from research groups that have been converging the best practices from both industry and academia make it clear that a galant future is upon us. Additionally, the many possibilities in applying these techniques in other fields (for instance, in Physics, with the enormous amount of data generated by laboratories such as the LHC, which urge Machine Learning to be analyzed), are limitless. These are all concrete proofs that &lt;strong&gt;human’s and machine’s creativity will guide humankind into our next paradigm&lt;/strong&gt;. &lt;/p&gt;</summary></entry></feed>
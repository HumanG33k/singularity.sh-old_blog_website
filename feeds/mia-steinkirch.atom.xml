<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>chmod +x singularity.sh</title><link href="http://bt3gl.github.io/" rel="alternate"></link><link href="http://bt3gl.github.io/feeds/mia-steinkirch.atom.xml" rel="self"></link><id>http://bt3gl.github.io/</id><updated>2019-04-20T09:00:00-04:00</updated><entry><title>8 Hints that Keanu Reeves is a Messiah</title><link href="http://bt3gl.github.io/8-hints-that-keanu-reeves-is-a-messiah.html" rel="alternate"></link><updated>2019-04-20T09:00:00-04:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2019-04-20:8-hints-that-keanu-reeves-is-a-messiah.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/keanu.jpg" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;I mean... Morpheus knew he was the chosen one. &lt;/p&gt;
&lt;h3&gt;1. He can walk-in on any Soundtrack&lt;/h3&gt;
&lt;p&gt;Just ask these &lt;a href="https://twitter.com/keanuwtm"&gt;40k followers&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;2. /r/KeanuBeingAwesome/&lt;/h3&gt;
&lt;p&gt;How do you get an entire &lt;a href="https://www.reddit.com/r/KeanuBeingAwesome/"&gt;reddit&lt;/a&gt; page? &lt;/p&gt;
&lt;h3&gt;3. He just anomalously helps Children Hospitals&lt;/h3&gt;
&lt;p&gt;Aw, &lt;a href="https://www.theepochtimes.com/hollywood-superstar-keanu-reeves-has-secretly-been-financing-countless-childrens-hospitals_2739162.html"&gt;such a nice heart&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;4. Guess who is in Cyberpunk 2077&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://io9.gizmodo.com/which-cyberpunk-keanu-reeves-protagonist-will-you-be-in-1835384364"&gt;Which Cyberpunk Keanu Reeves Protagonist Will You Be in the Future?&lt;/a&gt; &lt;/p&gt;
&lt;h3&gt;5. Finally, someone is standing with "being alone is freakin awesome"&lt;/h3&gt;
&lt;p&gt;&lt;img alt="cyberpunk" height="200" src="./cyberpunk/keanu2.jpg" width="300" /&gt;&lt;/p&gt;
&lt;h3&gt;6. I mean, The Matrix&lt;/h3&gt;
&lt;p&gt;Like, the best trilogy ever (after Back to the Future üíÅüèº‚Äç‚ôÄÔ∏è). &lt;/p&gt;
&lt;h3&gt;7. I mean, Point Break&lt;/h3&gt;
&lt;p&gt;Like, he learns to surf deadly waves in a few weeks ‚ò†Ô∏è. &lt;/p&gt;
&lt;h3&gt;8. I mean, Bill &amp;amp; Ted's Excellent Adventure&lt;/h3&gt;
&lt;p&gt;Like, the coolest time-traveler stoner-couple from the 90s (after Jay &amp;amp; Silent Bob üëæ). &lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;I dunno, some people are just special? üëÅ&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Aloha, bt3&lt;/strong&gt;&lt;/p&gt;</summary><category term="TheMatrix"></category><category term="love"></category><category term="troll"></category></entry><entry><title>You Never Have to Apologize For...</title><link href="http://bt3gl.github.io/you-never-have-to-apologize-for.html" rel="alternate"></link><updated>2019-03-29T09:00:00-04:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2019-03-29:you-never-have-to-apologize-for.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/sol.jpg" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Embrace your vulnerability.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Things we should never apologize for ;)&lt;/h2&gt;
&lt;h4&gt;1. Removing someone from our lives that repeatedly crosses our boundaries.&lt;/h4&gt;
&lt;h4&gt;2. Being who we are, and feeling our feelings.&lt;/h4&gt;
&lt;h4&gt;3. Trusting our instincts, even if we can't explain it.&lt;/h4&gt;
&lt;h4&gt;4. If we are not truly sorry.&lt;/h4&gt;
&lt;h4&gt;5. Quality me time (taking care of ourselves).&lt;/h4&gt;
&lt;h4&gt;6. Our opinion. There is no right or wrong &lt;em&gt;opinion&lt;/em&gt;, just different insights.&lt;/h4&gt;
&lt;h4&gt;7. Standing up for what we believe in.&lt;/h4&gt;
&lt;h4&gt;8. Living life the way we choose to, regardless of fitting in with other people's norms.&lt;/h4&gt;
&lt;h4&gt;9. Making decisions about our own future that don't do any harm to anyone.&lt;/h4&gt;
&lt;h4&gt;10. Being sensitive.&lt;/h4&gt;
&lt;p&gt;üíú&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Aloha, bt3&lt;/strong&gt;&lt;/p&gt;</summary><category term="mindfulness"></category><category term="TheGame"></category><category term="reality"></category></entry><entry><title>8 Favorite Drone videos filmed and edited by Me, Myself and I</title><link href="http://bt3gl.github.io/8-favorite-drone-videos-filmed-and-edited-by-me-myself-and-i.html" rel="alternate"></link><updated>2018-11-12T09:00:00-05:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2018-11-12:8-favorite-drone-videos-filmed-and-edited-by-me-myself-and-i.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/drone.jpg" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;So here is a story for you. A few years ago, after I left my career in Silicon Valley, I decided to &lt;em&gt;live life&lt;/em&gt; ü§ì. I spent almost two years being a full-time traveler (part-time remote-located engineer and part-time &lt;a href="https://www.instagram.com/innermost.limits.of.pure.fun"&gt;having-so-much-fun&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Here are my top drone videos from those wild times:&lt;/p&gt;
&lt;h2&gt;1. Hawai‚Äôi Aerial Reel&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/280851784" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;Living in Hawaii for almost 5 months in 4 different occasions freed my soul for good (whatever this means).&lt;/p&gt;
&lt;p&gt;The droning in this "reel" is so so beautiful, and it was so much fun to film.&lt;/p&gt;
&lt;p&gt;I will never forget those days üíñ.&lt;/p&gt;
&lt;h2&gt;2. Life is all about you &amp;amp; Not at all about you&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/285146753" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;I just love this song by &lt;a href="https://en.wikipedia.org/wiki/Zhu_(musician)"&gt;Zhu&lt;/a&gt;. I can listen to it over and over.&lt;/p&gt;
&lt;p&gt;But the droning in this video is also really special to me because it was my first summer in SoCal üå¥.&lt;/p&gt;
&lt;h2&gt;3. Santa Monica Daze&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/272891738" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;I can't even.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;S-O M-U-C-H F-E-E-L-Z.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thank you, Godspeed-Z.&lt;/p&gt;
&lt;h2&gt;4. Tiny Jaws&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/297056325" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;I always really wanted to go to Maui, it was itching.&lt;/p&gt;
&lt;p&gt;So I took two weeks off my job at Surfline in Huntington Beach and just traveled to &lt;a href="https://www.booking.com/hotel/us/hakuna-matata-lahaina.html"&gt;a random hostel in Lahaina&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It turned out that I was on the other side of the island from &lt;a href="https://en.wikipedia.org/wiki/Peahi,_Hawaii"&gt;Jaws&lt;/a&gt;, which was the place I really wanted to drone.&lt;/p&gt;
&lt;p&gt;I longboarded for hours with my drone in my backpack and a killer hot sun on my face.&lt;/p&gt;
&lt;p&gt;I almost died to find the place but it was soooo worth it üòé.&lt;/p&gt;
&lt;h2&gt;5. Surf &amp;amp; Chill in Malibu&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/271743314" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;Malibu :) (and some credit to Godspeed-Z).&lt;/p&gt;
&lt;p&gt;PS: I had a website miasteinkirch.com once and this was the opening (and 3k people loaded it? Waaaaat!)&lt;/p&gt;
&lt;h2&gt;6. Haleiwa Dreams&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/272116715" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;One of the best days of my life was when I was living in Haleiwa.&lt;/p&gt;
&lt;p&gt;Everything was magical and colorful.&lt;/p&gt;
&lt;p&gt;Surfing was so tight.&lt;/p&gt;
&lt;p&gt;This video is actually called mia's dreams ‚ú®.&lt;/p&gt;
&lt;h2&gt;7. Longboarding Haleiwa&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/269152192" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;OK, the surf sucks because there were no waves but this video still gives me the chills.&lt;/p&gt;
&lt;p&gt;I was 100% happy and this is not easy as a human in 2018.&lt;/p&gt;
&lt;p&gt;Plus, Post Malone wrote this song for me. ‚ò†Ô∏è&lt;/p&gt;
&lt;h2&gt;8. Go Skateboarding Day w/ Sierra Prescott&lt;/h2&gt;
&lt;iframe src="https://player.vimeo.com/video/274809384" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;p&gt;So I met this &lt;a href="https://www.instagram.com/sierra_prescott/?hl=en"&gt;crazy good and famous LA local skateboarder&lt;/a&gt; and we hung out üê∞.&lt;/p&gt;
&lt;p&gt;Good summer times with some badass LA gals.&lt;/p&gt;
&lt;h2&gt;Bonus: These ones are just hilariously bad and good, I don't know ü§∑üèº‚Äç‚ôÄ&lt;/h2&gt;
&lt;h4&gt;Skateboarding the Pipeline&lt;/h4&gt;
&lt;iframe src="https://player.vimeo.com/video/268763873" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;h4&gt;Skateboarding Poods&lt;/h4&gt;
&lt;iframe src="https://player.vimeo.com/video/265661224" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;h4&gt;Wake up High in Encinitas&lt;/h4&gt;
&lt;iframe src="https://player.vimeo.com/video/263762934" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;h4&gt;It‚Äôs Summer in the Pipe!&lt;/h4&gt;
&lt;iframe src="https://player.vimeo.com/video/269465467" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;h4&gt;Surfing Venice Beach&lt;/h4&gt;
&lt;iframe src="https://player.vimeo.com/video/271351879" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""&gt;&lt;/iframe&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;I dunno, have fun, work hard, be yourself, and stay rebel? ü§òüèº&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Aloha, bt3&lt;/strong&gt;&lt;/p&gt;</summary><category term="nomad"></category><category term="droning"></category><category term="love"></category></entry><entry><title>Thinking about Machine Learning Data Pipelines</title><link href="http://bt3gl.github.io/thinking-about-machine-learning-data-pipelines.html" rel="alternate"></link><updated>2018-05-16T04:19:00-04:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2018-05-16:thinking-about-machine-learning-data-pipelines.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/data_pip.png" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;Almost every day I have a new idea of some machine learning model for an app or some feature. Machine learning involves tasks that include data sourcing, data ingestion, data transformation, pre-processing data for use in training, training a model and hosting the model. Additionally, to get value out of machine learning models we need an architecture and process in place to repeatedly and consistently train new models and retrain existing models with new data.&lt;/p&gt;
&lt;p&gt;For example, for a movie dataset from an external source on the internet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If we are in AWS, we could upload it to &lt;strong&gt;S3&lt;/strong&gt; and then bring to &lt;strong&gt;Dynamo DB&lt;/strong&gt;. The data could be ingested as a one-time full-load as a batch or as a real-time stream.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There may be a need to do &lt;strong&gt;both batch and stream or just a batch or a stream&lt;/strong&gt;. In this case, the data could be &lt;strong&gt;full-loaded of data into Dynamo DB&lt;/strong&gt; and then &lt;strong&gt;stream new records into Kinesis stream using the Lambda function&lt;/strong&gt; as a source simulator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We could create a &lt;strong&gt;schema on the data stored in S3 and DynamoDB and perform ETL&lt;/strong&gt; on the data to prepare it for the machine learning process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we have an &lt;strong&gt;AWS S3 data lake&lt;/strong&gt; ready, we could use &lt;strong&gt;Amazon Sagemaker for model training and inference&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I start a little &lt;a href="https://github.com/bt3gl/Data-Pipelines"&gt;github repo for my personal research (dumps...) on Data Pipelines&lt;/a&gt; (PR your contribution!), in this post I add my little intro on the topic.&lt;/p&gt;
&lt;h2&gt;ETL: Extract, Transform, and Load&lt;/h2&gt;
&lt;p&gt;These three conceptual steps are how most data pipelines are designed and structured, serving as a blueprint for how raw data is transformed to analysis-ready data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt;: sensors wait for upstream data sources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transform&lt;/strong&gt;: business logic is applied (e.g. filtering, grouping, and aggregation to translate raw data into analysis-ready datasets).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load&lt;/strong&gt;: processed data is transported to a final destination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Airflow&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/airflow"&gt;Apache Airflow&lt;/a&gt; was a tool &lt;a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8"&gt;developed by Airbnb in 2014 and later open-sourced&lt;/a&gt;. It is a platform to programmatically author, schedule, and monitor workflows. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt;
&lt;p&gt;You can use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt;
&lt;p&gt;The key concepts are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DAG: a directed acyclic graph object that ties together all the tasks in a cohesive workflow and dictates the execution frequency (i.e. schedule).&lt;/li&gt;
&lt;li&gt;task: a unit of work to be executed that should be both atomic and idempotent. In Airflow there are two types of tasks: Operators and Sensors.&lt;/li&gt;
&lt;li&gt;operator: a specific type of work to be executed.&lt;/li&gt;
&lt;li&gt;sensor: a blocking task that runs until a condition is met or until it times out.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Airflow's architecture has the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;job definitions (in source control).&lt;/li&gt;
&lt;li&gt;CLI: to test, run, backfill, describe and clear parts of your DAGs.&lt;/li&gt;
&lt;li&gt;web application: to explore your DAGs definition, their dependencies, progress, metadata, and logs (built in Flask).&lt;/li&gt;
&lt;li&gt;metadata repository (in MySQL or Postgres): keeps track of task job statuses.&lt;/li&gt;
&lt;li&gt;array of workers: runs jobs task instances in a distributed fashion.&lt;/li&gt;
&lt;li&gt;scheduler: fires up the task instances that are ready.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is &lt;a href="https://gist.github.com/robert8138/c6e492d00cd7b7e7626670ba2ed32e6a"&gt;a very simple toy example of an Airflow job&lt;/a&gt; that simply prints the date in bash every day after waiting for one second to pass, after the execution date is reached:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DAG&lt;/span&gt;  &lt;span class="c"&gt;# Import the DAG class&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators.bash_operator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BashOperator&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators.sensors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TimeDeltaSensor&lt;/span&gt;

&lt;span class="n"&gt;default_args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2018&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;dag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DAG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;dag_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anatomy_of_a_dag&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;This describes my DAG&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;default_args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;default_args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;schedule_interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c"&gt;# This is a daily DAG.&lt;/span&gt;

&lt;span class="c"&gt;# t0 and t1 are examples of tasks created by instantiating operators&lt;/span&gt;
&lt;span class="n"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TimeDeltaSensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;task_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wait_a_second&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BashOperator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;task_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;print_date_in_bash&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;bash_command&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_upstream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Luigi&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Luigi data pipelining&lt;/a&gt; is Spotify's Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, etc. It also comes with Hadoop support built in.&lt;/p&gt;
&lt;p&gt;The basic units of Luigi are task classes that model an atomic ETL operation, in three parts: a requirements part that includes pointers to other tasks that need to run before this task, the data transformation step, and the output. All tasks can be feed into a final table (e.g. on Redshift) into one file.&lt;/p&gt;
&lt;p&gt;Here is &lt;a href="https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7"&gt;an example of a simple workflow in Luigi&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;luigi&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WritePipelineTask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/output_one.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pipeline&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;AddMyTask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/output_two.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;WritePipelineTask&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;decorated_line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;
            &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decorated_line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Airflow vs. Luigi&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Airflow&lt;/th&gt;
&lt;th&gt;Luigi&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web dashboard&lt;/td&gt;
&lt;td&gt;very nice&lt;/td&gt;
&lt;td&gt;minimal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Built in scheduler&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Separates output data and task state&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;calendar scheduling&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no, use cron&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parallelism&lt;/td&gt;
&lt;td&gt;yes, workers&lt;/td&gt;
&lt;td&gt;threads per workers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;finds new deployed tasks&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;persists state&lt;/td&gt;
&lt;td&gt;yes, to db&lt;/td&gt;
&lt;td&gt;sort of&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sync tasks to workers&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scheduling&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Learning References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/davidyakobovitch/data_science_resources"&gt;Data science resources&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/instacart/lore"&gt;Lorte data pipelining&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/incubator-airflow"&gt;Incubator Airflow data pipelining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udemy.com/airflow-basic-for-beginners/"&gt;Udemy's Airflow for Beginners&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jghoman/awesome-apache-airflow"&gt;Awesome Airflow Resources&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rolanddb/airflow-on-kubernetes"&gt;Airflow in Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/astronomer/astronomer"&gt;Astronomer: Airflow as a Service&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aws-samples/data-pipeline-samples/tree/master/samples"&gt;Data pipeline samples&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="GoogleCloudPlatform/DataflowTemplates"&gt;GCP Dataflow templates&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/binhnguyennus/awesome-scalability"&gt;Awesome Scalability: a lot of articles and resources on the subject&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Enterprise Solutions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/netflix-techblog/evolution-of-the-netflix-data-pipeline-da246ca36905"&gt;Netflix data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UC00QATOrSH4K2uOljTnnaKw"&gt;Netlix data videos&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html"&gt;Yelp data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.gusto.com/building-a-data-informed-culture/"&gt;Gusto data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83."&gt;500px data pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/ml-workflows.html"&gt;Twitter data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@zhaojunzhang/building-data-infrastructure-in-coursera-15441ebe18c2"&gt;Coursera data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/"&gt;Cloudfare data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee"&gt;Pandora data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@damesavram/running-airflow-on-heroku-ed1d28f8013d"&gt;Heroku data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zillow.com/data-science/airflow-at-zillow/"&gt;Zillow data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166"&gt;Airbnb data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3"&gt;Walmart data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8"&gt;Robinwood data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff"&gt;Lyft data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/vananth22/operating-data-pipeline-with-airflow-at-slack"&gt;Slack data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@RemindEng/beyond-a-redshift-centric-data-model-1e5c2b542442"&gt;Remind data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/wish-engineering/scaling-analytics-at-wish-619eacb97d16"&gt;Wish data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://databricks.com/blog/2017/03/31/delivering-personalized-shopping-experience-apache-spark-databricks.html"&gt;Databrick data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Courses &amp;amp; Videos&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/lecture/big-data-integration-processing/big-data-processing-pipelines-c4Wyd"&gt;Coursera's Big Data Pipeline course&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=3JYDT8lap5U"&gt;Industrial Machine Learning Talk&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Aloha, bt3&lt;/strong&gt;&lt;/p&gt;</summary><category term="python"></category><category term="data"></category><category term="airflow"></category><category term="etl"></category><category term="luigi"></category></entry><entry><title>System Designing with End-to-end Applications in AWS</title><link href="http://bt3gl.github.io/system-designing-with-end-to-end-applications-in-aws.html" rel="alternate"></link><updated>2018-04-15T09:00:00-04:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2018-04-15:system-designing-with-end-to-end-applications-in-aws.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/city.jpg" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Systems_design"&gt;System design&lt;/a&gt;&lt;/strong&gt; is a really interesting open-ended subject in software engineering, and the way you tackle it tends to be related to how much experience you have in the industry. Every time I learn about a new application or service, I try to imagine its design. I love to learn when and why my approach was off :).&lt;/p&gt;
&lt;p&gt;One of my favorite projects in my software career was when I had to implement an end-to-end application to perform some manipulations (with &lt;a href="https://ffmpeg.org/"&gt;FFMPEG&lt;/a&gt;) on surf video clips that were available in some cloud storage resource (say, &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html"&gt;S3 buckets&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The app I wrote was a Python software running in &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt;, which would be triggered by messages from an &lt;a href="https://aws.amazon.com/sqs/"&gt;SQS&lt;/a&gt; queue. The function would then retrieve some specific clip, trim and edit it, and then it would 1) save the new clips to a destination bucket, 2) add their metadata to a &lt;a href="https://www.mongodb.com/"&gt;MongoDB&lt;/a&gt; database, and 3) send a notification to a topic in an &lt;a href="https://aws.amazon.com/sns/"&gt;SNS&lt;/a&gt; service, so that the front end bit could pick it up.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Anonymous_function"&gt;Lambda functions&lt;/a&gt; are simply awesome because they abstract several layers in your design (e.g. serverless, anonymous) and they are super easy to set up. But I am getting ahead myself, let's talk a little more about high-level system design!&lt;/p&gt;
&lt;p&gt;Whenever you are designing a system, these are some first questions to think about:&lt;/p&gt;
&lt;h2&gt;Scope of the System&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User cases&lt;/strong&gt;: Who is going to use it? How are they going to use it?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Amount of traffic, amount of data, the scale of the system (e.g. requests per second, requests types, data written per second, data read per second), special system requirements (e.g. multi-threading, read or write-oriented, etc.)?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; High-level architecture design&lt;/strong&gt;: Application service layer, different services required, data storage layer?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Understanding bottlenecks&lt;/strong&gt;: Do we need a load balancer and many instances behind it to handle user requests? Is data large enough so that you need to distribute your database on multiple machines? What are the downsides from doing that?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User interface&lt;/strong&gt;: Is this a full web app, with a web interface? Or just a RESTful API?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Availability &amp;amp; Reliability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How things can fail, especially in a distributed environment?&lt;/li&gt;
&lt;li&gt;How to design a system to cope with network failures?&lt;/li&gt;
&lt;li&gt;Should the system be 100% reliable? &lt;/li&gt;
&lt;li&gt;Do we need high availability? &lt;/li&gt;
&lt;li&gt;Do we need redundancy (e.g. multiple replicas of services running in the system, so that if a few services die down the system still remains available and running)?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Scaling&lt;/h2&gt;
&lt;p&gt;Simply put, you can scale your system &lt;strong&gt;vertically&lt;/strong&gt; (e.g. adding more CPU, RAM to your existing machine), or &lt;strong&gt;horizontally&lt;/strong&gt; (adding more machines into your pool of resources).&lt;/p&gt;
&lt;h3&gt;Database&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Do we need a &lt;strong&gt;relational database&lt;/strong&gt; that is based on tabular design (e.g. MySQL) or &lt;strong&gt;non-relational NoSQL&lt;/strong&gt;, which is document-based (e.g. MongoDB)?&lt;/li&gt;
&lt;li&gt;Do we need &lt;strong&gt;Database replication&lt;/strong&gt; (e.g. the frequent copying data from a database in one server to a database in another, so that all users share the same level of information)?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Caching and Fast lookups&lt;/h3&gt;
&lt;p&gt;There are several types of &lt;strong&gt;caches&lt;/strong&gt; that can be used in your application: &lt;strong&gt;application caching&lt;/strong&gt;, &lt;strong&gt;database caching&lt;/strong&gt;, &lt;strong&gt;in-memory caches&lt;/strong&gt;, &lt;strong&gt;global cache&lt;/strong&gt;, &lt;strong&gt;distributed cache&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One example of a popular open source cache is &lt;a href="http://memcached.org/"&gt;Memcached&lt;/a&gt; (which can work both as a local cache and distributed cache). Memcached is used in many large web sites, and even though it can be very powerful, it is simply an in-memory key value store, optimized for arbitrary data storage and fast lookups.&lt;/p&gt;
&lt;h3&gt;Load balancing and Redundancy&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Load balancers&lt;/strong&gt; are a principal part of any architecture, as their role is to distribute load across a set of nodes responsible for servicing requests. Their main purpose is to handle a lot of simultaneous connections and route those connections to one of the request nodes, allowing the system to scale to service more requests by just adding nodes.&lt;/p&gt;
&lt;p&gt;Load balancers also provide the critical function of being able to test the health of a node, such that if a node is unresponsive or over-loaded, taking advantage of the redundancy of different nodes in your system.&lt;/p&gt;
&lt;h3&gt;Queues and Asynchronous requests&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Queues&lt;/strong&gt; are a common way to bring asynchrony into your system, for instance, in the cases when some tasks (e.g. writes) may take a long time. This helps to achieve performance and availability.&lt;/p&gt;
&lt;p&gt;A queue is as simple as it sounds: a task comes in, is added to the queue and then workers pick up the next task, providing an abstraction of a client's request and its response.&lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Back to my end-to-end Application&lt;/h1&gt;
&lt;p&gt;A good way to illustrate system design is looking at a real end-to-end application deployed at AWS. &lt;/p&gt;
&lt;p&gt;For the surf clips example I mentioned above, the architecture involves API event notifications, S3 buckets, an SNS topic, an SQS queue, and a Lambda function.&lt;/p&gt;
&lt;p&gt;The SQS queue stores the event for asynchronous processing (e.g. requesting certain clip edit). The Lambda function parses the event (e.g. run FFMPEG), and sends a notification message to the SNS topic (e.g. "clip is ready"). A topic groups together messages of the same type which might be of interest to a set of subscribers (e.g. "video_production" for front end API):&lt;/p&gt;
&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/aws.png" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;Together with the (Python) app source code to be run as a Lambda function, you can set all the components of your system in &lt;a href="https://www.terraform.io/"&gt;Terraform&lt;/a&gt;, in a very neat way. &lt;/p&gt;
&lt;p&gt;For instance, our Lambda function would be allocated by some code like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;aws_lambda_function&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;function_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;FUNCTION_NAME&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="n"&gt;runtime&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python2.7&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;timeout&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
  &lt;span class="n"&gt;s3_bucket&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;BUCKET&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;ZIP&lt;/span&gt; &lt;span class="nx"&gt;IS&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="n"&gt;s3_key&lt;/span&gt;        &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;${var.producer_zip}&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;handler&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;CODE HANDLER&amp;gt;&lt;/span&gt;
&lt;span class="s2"&gt;  role          = &amp;quot;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;aws_iam_role.lambda_role.arn&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;  memory_size   = 1024&lt;/span&gt;

&lt;span class="s2"&gt;  vpc_config = {&lt;/span&gt;
&lt;span class="s2"&gt;      ...&lt;/span&gt;
&lt;span class="s2"&gt;  }&lt;/span&gt;

&lt;span class="s2"&gt;  environment {&lt;/span&gt;
&lt;span class="s2"&gt;    variables = {&lt;/span&gt;
&lt;span class="s2"&gt;        ...&lt;/span&gt;
&lt;span class="s2"&gt;    }&lt;/span&gt;
&lt;span class="s2"&gt;  }&lt;/span&gt;
&lt;span class="s2"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Similarly, other AWS resources such as SQS, SNS, their topics, and buckets, such as the final destination S3 bucket (where the final clips would be stored), would have Terraform code snippets as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;aws_s3_bucket&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;bucket_name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="n"&gt;acl&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;public-read&amp;quot;&lt;/span&gt;
  &lt;span class="nx"&gt;cors_rule&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;allowed_headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    allowed_methods = &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;GET&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;HEAD&amp;quot;&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    allowed_origins = &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    max_age_seconds = 86400
  }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, all you need is a couple of commands (such as &lt;code&gt;terraform apply&lt;/code&gt;) and your system would be up and running. &lt;/p&gt;
&lt;p&gt;This was a high-level overview of how you would start designing and implementing real-world end-to-end applications, where &lt;strong&gt;design meets code and infrastructure as a code&lt;/strong&gt;. Pretty awesome to live in 2019.&lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Learning References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/terraform-providers/terraform-provider-aws"&gt;Terraform Provider AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.lecloud.net/tagged/scalability"&gt;Scalability for Dummies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aosabook.org/en/distsys.html"&gt;Scalable Web Architecture and Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html"&gt;Scalable System Design Patterns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/hulu-tech-blog/the-challenges-of-live-linear-video-ingest-part-one-live-versus-on-demand-system-requirements-89238f3af4f6"&gt;Hulu's "The Challenges of Live Video"&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.u-cursos.cl/usuario/f133dab21b6cbf814b4607124f431358/mi_blog/r/head_first_design_patterns.pdf"&gt;Heads First - Design Pattern&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;
Aloha, bt3
&lt;/strong&gt;&lt;/p&gt;</summary><category term="aws"></category><category term="scalability"></category><category term="queues"></category><category term="lambda"></category><category term="sqs"></category><category term="mobgodb"></category></entry></feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>chmod +x singularity.sh</title><link href="http://bt3gl.github.io/" rel="alternate"></link><link href="http://bt3gl.github.io/feeds/data.atom.xml" rel="self"></link><id>http://bt3gl.github.io/</id><updated>2019-02-16T04:19:00-05:00</updated><entry><title>Data Pipelines in Python</title><link href="http://bt3gl.github.io/data-pipelines-in-python.html" rel="alternate"></link><updated>2019-02-16T04:19:00-05:00</updated><author><name>Mia Steinkirch</name></author><id>tag:bt3gl.github.io,2019-02-16:data-pipelines-in-python.html</id><summary type="html">&lt;p&gt;&lt;img alt="cyberpunk" height="270px" src="./cyberpunk/data_pip.png" width="390px" /&gt;&lt;/p&gt;
&lt;p&gt;Almost every day I have a new idea to run in some machine learning model or problem, and this field is only getting bigger. As I start a little &lt;a href="https://github.com/bt3gl/Data-Pipelines"&gt;github repo for any  research dump on Data Pipelines&lt;/a&gt; (PR your contribution!), in this post I add my little summary on how it goes for Python.&lt;/p&gt;
&lt;p&gt;To continue to get value out of machine learning models one needs an architecture and process in place to repeatedly and consistently train new models and retrain existing models with new data.&lt;/p&gt;
&lt;p&gt;Machine learning involves tasks that includes data sourcing, data ingestion, data transformation, pre-processing data for use in training, training a model and hosting the model.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a movie dataset from external source on internet: bring it to S3 and upload it into Dynamo DB. The data can be ingested as a one time full-load as a batch or as a real-time stream of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There may be a need to do both batch and stream or just a batch or a stream. The data can be full-loaded of data into Dynamo DB and then stream new records into Kinesis stream using the Lambda function as a source simulator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One can create a schema on the data stored in S3 and DynamoDB and perform ETL on the data to prepare it for the machine learning process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If one has an AWS S3 data lake ready, one can use Amazon Sagemaker for model training and inference.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ETL: Extract, Transform, and Load&lt;/h2&gt;
&lt;p&gt;These three conceptual steps are how most data pipelines are designed and structured, serving as a blueprint for how raw data is transformed to analysis-ready data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extract: sensors wait for upstream data sources.&lt;/li&gt;
&lt;li&gt;Transform: business logic is applied (e.g. filtering, grouping, and aggregation to translate raw data into analysis-ready datasets).&lt;/li&gt;
&lt;li&gt;Load: processed data is transported to a final destination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Airflow&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/airflow"&gt;Apache Airflow&lt;/a&gt; was a tool &lt;a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8"&gt;developed by Airbnb in 2014 and later open-sourced&lt;/a&gt;. It is a platform to programmatically author, schedule, and monitor workflows. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt;
&lt;p&gt;Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt;
&lt;p&gt;The key concepts are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DAG: a directed acyclic graph object that ties together all the tasks in a cohesive workflow and dictates the execution frequency (i.e. schedule).&lt;/li&gt;
&lt;li&gt;task: a unit of work to be executed that should be both atomic and idempotent. In Airflow there are two types of tasks: Operators and Sensors.&lt;/li&gt;
&lt;li&gt;operator: a specific type of work to be executed.&lt;/li&gt;
&lt;li&gt;sensor: a blocking task that runs until a condition is met or until it times out.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Airflow's architecture has the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;job definitions (in source control).&lt;/li&gt;
&lt;li&gt;CLI: to test, run, backfill, describe and clear parts of your DAGs.&lt;/li&gt;
&lt;li&gt;web application: to explore your DAGs definition, their dependencies, progress, metadata and logs (built in Flask).&lt;/li&gt;
&lt;li&gt;metadata repository (in MySQL or Postgres): keeps track of task job statuses.&lt;/li&gt;
&lt;li&gt;array of workers: runs jobs task instances in a distributed fashion.&lt;/li&gt;
&lt;li&gt;scheduler: fires up the task instances that are ready.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is &lt;a href="https://gist.github.com/robert8138/c6e492d00cd7b7e7626670ba2ed32e6a"&gt;a very simple toy example of an Airflow job&lt;/a&gt; that simply prints the date in bash every day after waiting for one second to pass, after the execution date is reached:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DAG&lt;/span&gt;  &lt;span class="c"&gt;# Import the DAG class&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators.bash_operator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BashOperator&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators.sensors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TimeDeltaSensor&lt;/span&gt;

&lt;span class="n"&gt;default_args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2018&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;dag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DAG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;dag_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anatomy_of_a_dag&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;This describes my DAG&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;default_args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;default_args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;schedule_interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c"&gt;# This is a daily DAG.&lt;/span&gt;

&lt;span class="c"&gt;# t0 and t1 are examples of tasks created by instantiating operators&lt;/span&gt;
&lt;span class="n"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TimeDeltaSensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;task_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wait_a_second&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BashOperator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;task_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;print_date_in_bash&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;bash_command&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dag&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_upstream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Luigi&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Luigi data pipelining&lt;/a&gt; is Spotify's Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.&lt;/li&gt;
&lt;li&gt;The basic unit of Luigi are task classes that model an atomic ETL operation, in three parts: a requirements part that includes pointers to other tasks that need to run before this task, the data transformation step, and the output. &lt;/li&gt;
&lt;li&gt;All tasks can be feed into a final table (e.g. on Redshift) into one file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is &lt;a href="https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7"&gt;an example of a simple workflow in Luigi&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;luigi&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WritePipelineTask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/output_one.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pipeline&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;AddMyTask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/output_two.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;WritePipelineTask&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;decorated_line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;
            &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decorated_line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Airflow vs. Luigi&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Airflow&lt;/th&gt;
&lt;th&gt;Luigi&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web dashboard&lt;/td&gt;
&lt;td&gt;very nice&lt;/td&gt;
&lt;td&gt;minimal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Built in scheduler&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Separates output data and task state&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;calendar scheduling&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no, use cron&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parallelism&lt;/td&gt;
&lt;td&gt;yes, workers&lt;/td&gt;
&lt;td&gt;threads per workers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;finds new deployed tasks&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;persists state&lt;/td&gt;
&lt;td&gt;yes, to db&lt;/td&gt;
&lt;td&gt;sort of&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sync tasks to workers&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scheduling&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Learning References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/davidyakobovitch/data_science_resources"&gt;Data science resources&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/instacart/lore"&gt;Lorte data pipelining&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/incubator-airflow"&gt;Incubator Airflow data pipelining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udemy.com/airflow-basic-for-beginners/"&gt;Udemy's Airflow for Beginners&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jghoman/awesome-apache-airflow"&gt;Awesome Airflow Resources&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rolanddb/airflow-on-kubernetes"&gt;Airflow in Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/astronomer/astronomer"&gt;Astronomer: Airflow as a Service&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aws-samples/data-pipeline-samples/tree/master/samples"&gt;Data pipeline samples&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="GoogleCloudPlatform/DataflowTemplates"&gt;GCP Dataflow templates&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;Classic Big Data Pipelines&lt;/h2&gt;
&lt;p&gt;Major constituents of a big data analytic pipeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The messaging system&lt;/li&gt;
&lt;li&gt;Distribution of messages to various nodes for further processing.&lt;/li&gt;
&lt;li&gt;Analytic processing, to derive inferences from data (e.g. application of machine learning of data).&lt;/li&gt;
&lt;li&gt;Data storage system for storing results.&lt;/li&gt;
&lt;li&gt;Interfaces or consumption of results data (e.g. visualization tools, alerts, etc).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Real-time tracking and keeping results accurately up to date can be satisfied by building a lambda architecture. Lambda architecture takes advantage of both batch and stream-processing methods. It attempts to balance throughput, latency and fault-tolerance by using batch processing, while simultaneously using real-time stream processing to provide views of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Component parts can be dynamically managed using Docker. Along with docker-compose file and/or skaffold, multi-containers application can be orchestred and the service configuration can be versioned for easy deploy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Messaging Platforms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stream processing allows processing data in real time (optimizing latency of batch processing and of stream processing. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Apache Kafka Clusters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Kafka is a high-throughput, distributed, publish-subscribe messaging system. &lt;/li&gt;
&lt;li&gt;Data can be ingested from multiple sources into Kafka, before passing it to compute and storage systems.&lt;/li&gt;
&lt;li&gt;Kafka provides two mechanisms: Producer and Listener. The API writes the data to the producer, ensuring it handles high volume and high frequency data.&lt;/li&gt;
&lt;li&gt;Another advantage of using Kafka is the  mapping of use cases of each queue. The designer can design the queues to be separated by use cases, thus keeping the processing logic to a required minimum. We need not write unnecessary code to handle those use cases, which will never arrive in a queue.&lt;/li&gt;
&lt;li&gt;One can add a secondary Kafka sub-system, predictive modeling with Apache Spark, and Elasticsearch.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Kinesis Firehouse&lt;/h4&gt;
&lt;h2&gt;Compute Engines/Ingestion&lt;/h2&gt;
&lt;h4&gt;Apache Spark&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spark Streaming can be used here to ensure that messages received are spread out on the cluster and processed efficiently. &lt;/li&gt;
&lt;li&gt;Other notable advantages of this is that a processing time window can be configured as needed.&lt;/li&gt;
&lt;li&gt;Fast and scalable solution as it employs in memory architecture.&lt;/li&gt;
&lt;li&gt;Spark is a faster faster than Hadoop's MapReduce architecture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Hadoop MapReduce&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop Distributed File System (HDFS) is a  popular data storage system.&lt;/li&gt;
&lt;li&gt;Distributed File Systems are client/server-based application that allows clients to access and process data stored on the server as if it were on their own computer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Warehouse/Stores&lt;/h2&gt;
&lt;h3&gt;Redshift&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQL interfaces, and the ease to process petabytes of data.&lt;/li&gt;
&lt;li&gt;Easy to maintain database that is optimized for analytical queries such as joins and aggregations&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;S3 or any  Cloud Filesystems&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Underline data lake, backups, redundancy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Hive&lt;/h4&gt;
&lt;h4&gt;RDS&lt;/h4&gt;
&lt;h4&gt;EMR&lt;/h4&gt;
&lt;h2&gt;Analytics and visualizations&lt;/h2&gt;
&lt;h4&gt;Periscope Data&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A lightweight analytics tool where you can write SQL to create charts. It's a full featured dashboard tool.&lt;/li&gt;
&lt;li&gt;Periscope can connect to data sources (e.g. Redshift) easily as they are both cloud solutions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Analytics and visualizations Learning References:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mode.com/sql-tutorial/introduction-to-sql/"&gt;Introduction to SQL&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.khanacademy.org/computing/computer-programming/sql"&gt;Khan's SQL course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Alerting&lt;/h2&gt;
&lt;h4&gt;Elastalert&lt;/h4&gt;
&lt;hr /&gt;
&lt;h3&gt;Learning References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/binhnguyennus/awesome-scalability"&gt;Awesome Scalability: a lot of articles and resources on the subject&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Enterprise Solutions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/netflix-techblog/evolution-of-the-netflix-data-pipeline-da246ca36905"&gt;Netflix data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UC00QATOrSH4K2uOljTnnaKw"&gt;Netlix data videos&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html"&gt;Yelp data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.gusto.com/building-a-data-informed-culture/"&gt;Gusto data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83."&gt;500px data pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/ml-workflows.html"&gt;Twitter data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@zhaojunzhang/building-data-infrastructure-in-coursera-15441ebe18c2"&gt;Coursera data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/"&gt;Cloudfare data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee"&gt;Pandora data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@damesavram/running-airflow-on-heroku-ed1d28f8013d"&gt;Heroku data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zillow.com/data-science/airflow-at-zillow/"&gt;Zillow data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166"&gt;Airbnb data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3"&gt;Walmart data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8"&gt;Robinwood data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff"&gt;Lyft data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/vananth22/operating-data-pipeline-with-airflow-at-slack"&gt;Slack data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@RemindEng/beyond-a-redshift-centric-data-model-1e5c2b542442"&gt;Remind data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/wish-engineering/scaling-analytics-at-wish-619eacb97d16"&gt;Wish data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://databricks.com/blog/2017/03/31/delivering-personalized-shopping-experience-apache-spark-databricks.html"&gt;Databrick data pipeline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Courses &amp;amp; Videos&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/lecture/big-data-integration-processing/big-data-processing-pipelines-c4Wyd"&gt;Coursera's Big Data Pipeline course&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=3JYDT8lap5U"&gt;Industrial Machine Learning Talk&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Aloha, bt3&lt;/strong&gt;&lt;/p&gt;</summary><category term="python"></category><category term="data"></category><category term="airflow"></category><category term="etl"></category><category term="luigi"></category></entry></feed>